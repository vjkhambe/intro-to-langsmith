{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c100556-9248-4e9d-a2e6-a0977a4b8c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web page  https://docs.smith.langchain.com/sitemap.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#####################################################################################################################################################################################| 219/219 [00:21<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded web page  https://docs.smith.langchain.com/sitemap.xml\n",
      "Loading web page  https://dspy.ai/sitemap.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#######################################################################################################################################################################################| 81/81 [00:08<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded web page  https://dspy.ai/sitemap.xml\n",
      "Vector DB is has setup now from path =  ./temp/vectorstore.union.parquet\n",
      "retriever  tags=['SKLearnVectorStore', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.sklearn.SKLearnVectorStore object at 0x31b3d2110> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from llmhelper import get_llm\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_vector_db_retriever(llm_model, documents):\n",
    "    persist_path = os.path.join(\"./temp/\", \"vectorstore.union.parquet\")\n",
    "    embd = OllamaEmbeddings(model=llm_model.model)\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        print(\"Vector DB is already initialized.\")\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "        \n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    print(\"Vector DB is has setup now from path = \", persist_path)\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_model(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_model(messages: List[dict]) -> str:\n",
    "    return llm_client.invoke(messages)\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response\n",
    "\n",
    "def load_website(): \n",
    "    web_paths = [] \n",
    "    web_paths.append(\"https://docs.smith.langchain.com/sitemap.xml\")\n",
    "    web_paths.append(\"https://dspy.ai/sitemap.xml\")\n",
    "    documents = []\n",
    "    for path in web_paths: \n",
    "        ls_docs_sitemap_loader = SitemapLoader(web_path=path,show_progress=True)\n",
    "        print(\"Loading web page \", path)\n",
    "        ls_docs = ls_docs_sitemap_loader.load()\n",
    "        print(\"Loaded web page \", path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=0)\n",
    "        doc_splits = text_splitter.split_documents(ls_docs)\n",
    "        documents.extend(doc_splits)\n",
    "    return documents\n",
    "\n",
    "llm_client = get_llm()\n",
    "nest_asyncio.apply()\n",
    "documents = load_website()\n",
    "retriever = get_vector_db_retriever(llm_model=llm_client, documents=documents) \n",
    "print(\"retriever \" , retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3066935f-d646-4f8c-aaa5-df6fa187a690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To enable tracing, you need to set the following environment variables:\\n\\n- OTEL_EXPORTER_OTLP_ENDPOINT with the LangSmith API endpoint URL\\n- OTEL_EXPORTER_OTLP_HEADERS with your LangSmith API key and optional project name.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what environment variables need to set for tracing?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e805ce-bd5f-49b7-9cfa-9364ac19b3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The DSPy framework is a Python-based programmatic framework designed to help developers build, train, and deploy large language models (LMs). It allows users to define their tasks, pipeline, and parameters using Python code, and provides features like assertions and suggestions to automate the process. The framework also supports various built-in modules for different prompting techniques, such as chain of thought or React.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is DSPy framework for programming?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8f37ef-e169-4005-95af-ea6f32b83efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High-quality output for an LLM-as-a-judge evaluator would be examples where the model demonstrates clear, accurate, and relevant routing of user intentions into the correct path (\"refund\" or \"question answering\"), with minimal errors or confusion. The output should also demonstrate a good understanding of nuances in language, such as subtleties in intent and context. This could involve evaluating how well the LLM handles edge cases or ambiguous inputs.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is high-quality outputs?\"\n",
    "langsmith_rag(question) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7161d64-5cf5-4447-9893-5ce3c4520b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The context provided discusses various aspects of RAG applications and LLMs, but there's no mention of Cross-LM Compatibility in DSPy.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is Cross-LM Compatibility in DSPy? \" \n",
    "langsmith_rag(question) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0a31a-03b1-4df3-b957-3299f512fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
