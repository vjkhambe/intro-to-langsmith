{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, an evaluator judges an invocation of your LLM application against a reference example, and returns an evaluation score.\n",
    "\n",
    "In LangSmith evaluators, we represent this process as a function that takes in a Run (representing the LLM app invocation) and an Example (representing the data point to evaluate), and returns Feedback (representing the evaluator's score of the LLM app invocation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluator](../../images/evaluator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"label\")\n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference).\n",
    "\n",
    "Here is an example of how you might define an LLM-as-judge evaluator with structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from llmhelper import get_llm\n",
    "\n",
    "client = get_llm()\n",
    "\n",
    "class Similarity_Score(BaseModel):\n",
    "    similarity_score: int = Field(description=\"Semantic similarity score between 1 and 10, where 1 means unrelated and 10 means identical.\")\n",
    "\n",
    "# NOTE: This is our evaluator\n",
    "def compare_semantic_similarity(inputs: dict, reference_outputs: dict, outputs: dict):\n",
    "    input_question = inputs[\"question\"]\n",
    "    reference_response = reference_outputs[\"output\"]\n",
    "    run_response = outputs[\"output\"]\n",
    "    messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n",
    "                    \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n",
    "                    \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n",
    "        ]\n",
    "    completion = client.invoke(\n",
    "        messages\n",
    "        #,response_format=Similarity_Score\n",
    "    )\n",
    "    print(\"LLM output => \" , completion)\n",
    "    similarity_score = completion.choices[0].message.parsed\n",
    "    return {\"score\": similarity_score.similarity_score, \"key\": \"similarity\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out!\n",
    "\n",
    "NOTE: We purposely made this answer wrong, so we expect to see a low score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output =>  Based on the comparison of meanings, I would score the similarity between the two responses as 8 out of 10.\n",
      "\n",
      "The Reference Response explicitly states that LangSmith is natively integrated with LangChain, which establishes a clear connection between the two concepts. The inclusion of LangGraph as an additional related concept adds to the coherence and accuracy of the reference response.\n",
      "\n",
      "In contrast, the New Response denies any integration between LangSmith and LangChain, using strong language (\"NOT integrated\") that suggests a complete disconnection between the two. While this response is grammatically correct, its meaning is quite different from the Reference Response, making it less similar in terms of semantic meaning.\n",
      "\n",
      "The only reason I wouldn't score it as 10/10 is that the New Response does provide some information about LangSmith (that it's not integrated with LangChain), which is a related concept. However, the way this information is presented and the overall tone of the response make it quite different from the Reference Response in terms of meaning and emphasis.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# From Run\u001b[39;00m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo, LangSmith is NOT integrated with LangChain.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_semantic_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSemantic similarity score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mcompare_semantic_similarity\u001b[0;34m(inputs, reference_outputs, outputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     27\u001b[0m     messages\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#,response_format=Similarity_Score\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM output => \u001b[39m\u001b[38;5;124m\"\u001b[39m , completion)\n\u001b[0;32m---> 31\u001b[0m similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mparsed\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: similarity_score\u001b[38;5;241m.\u001b[39msimilarity_score, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "# From Dataset Example\n",
    "inputs = {\n",
    "  \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "}\n",
    "reference_outputs = {\n",
    "  \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "}\n",
    "\n",
    "\n",
    "# From Run\n",
    "outputs = {\n",
    "  \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity(inputs, reference_outputs, outputs)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define evaluators using Run and Example directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def compare_semantic_similarity_v2(root_run: Run, example: Example):\n",
    "    input_question = example[\"inputs\"][\"question\"]\n",
    "    reference_response = example[\"outputs\"][\"output\"]\n",
    "    run_response = root_run[\"outputs\"][\"output\"]\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n",
    "                    \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n",
    "                    \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n",
    "        ],\n",
    "        response_format=Similarity_Score,\n",
    "    )\n",
    "\n",
    "    similarity_score = completion.choices[0].message.parsed\n",
    "    return {\"score\": similarity_score.similarity_score, \"key\": \"similarity\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_run = {\n",
    "  \"name\": \"Sample Run\",\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n",
    "  },\n",
    "  \"is_root\": True,\n",
    "  \"status\": \"success\",\n",
    "  \"extra\": {\n",
    "    \"metadata\": {\n",
    "      \"key\": \"value\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sample_example = {\n",
    "  \"inputs\": {\n",
    "    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"dataset_split\": [\n",
    "      \"AI generated\",\n",
    "      \"base\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity_v2(sample_run, sample_example)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
